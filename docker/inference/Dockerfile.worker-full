#https://github.com/yk/text-generation-inference/pkgs/container/text-generation-inference
#FROM ghcr.io/yk/text-generation-inference:latest

FROM ghcr.io/huggingface/text-generation-inference:1.4

# Install required dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    git

# Clean up APT when done
RUN apt-get clean && rm -rf /var/lib/apt/lists/*

# Avoid fragmentation. This setting can help by allowing PyTorch to allocate memory in expandable segments, potentially reducing fragmentation and improving memory usage.
ENV PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
ENV CUDA_LAUNCH_BLOCKING=1
ENV TORCH_USE_CUDA_DSA=1

ENV PATH=/usr/lib/:/usr/lib/x86_64-linux-gnu:$PATH
ENV LD_LIBRARY_PATH=/usr/lib/:/usr/local/nvidia/lib:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH

ARG MODULE="inference"
ARG SERVICE="worker"

ARG APP_RELATIVE_PATH="${MODULE}/${SERVICE}"

WORKDIR /worker

RUN conda create -n worker python=3.11 -y

COPY ./oasst-shared /tmp/oasst-shared
RUN pip install /tmp/oasst-shared

COPY ./${APP_RELATIVE_PATH}/requirements.txt .
RUN pip install -r requirements.txt

COPY ./${APP_RELATIVE_PATH}/*.py .
COPY ./${APP_RELATIVE_PATH}/worker_full_main.sh /entrypoint.sh

ENV MODEL_CONFIG_NAME="distilgpt2"
ENV NUM_SHARDS="1"

# These are upper bounds for the inference server.
ENV MAX_INPUT_LENGTH="514"
ENV MAX_TOTAL_TOKENS="8192"

ENV BACKEND_URL="ws://localhost:8000"

ENTRYPOINT ["/entrypoint.sh"]
